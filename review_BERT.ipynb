{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1XncKfbSRZwqPhuOEssxMk35hjt6CQHaA","authorship_tag":"ABX9TyPv7hW2s/Ey3xqZNLqRZrR5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"32c51f3aa521454dbfbf0730dbad251a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a9683202c9df46c09792e94a932aaf54","IPY_MODEL_9f302c2608604de2978b88961a9b0d9d","IPY_MODEL_e49f0eeb0b274a6cb19cad2ac910b28c"],"layout":"IPY_MODEL_ee2aa3442d53498f99ed6ee218621040"}},"a9683202c9df46c09792e94a932aaf54":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d2dcd8a40ed479296d4a207ac5a10d0","placeholder":"​","style":"IPY_MODEL_9722e952a4cc49deb1cfe46ad0f73000","value":"tokenizer_config.json: 100%"}},"9f302c2608604de2978b88961a9b0d9d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_306180ab90c24cb08a07d0fade54ea8c","max":432,"min":0,"orientation":"horizontal","style":"IPY_MODEL_131f5d857e254e8fbaf667b1e9ad75de","value":432}},"e49f0eeb0b274a6cb19cad2ac910b28c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_83c29790a92b4df7a149cc6c363cc02c","placeholder":"​","style":"IPY_MODEL_8d5de93162054eafa5ff484707e6d627","value":" 432/432 [00:00&lt;00:00, 12.2kB/s]"}},"ee2aa3442d53498f99ed6ee218621040":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d2dcd8a40ed479296d4a207ac5a10d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9722e952a4cc49deb1cfe46ad0f73000":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"306180ab90c24cb08a07d0fade54ea8c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"131f5d857e254e8fbaf667b1e9ad75de":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"83c29790a92b4df7a149cc6c363cc02c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d5de93162054eafa5ff484707e6d627":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"564ae84188c24b66bd86427b1281eaa2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6e24c15ccc7940b2b2d7a2c80fc0eeb1","IPY_MODEL_d328f8581a624833b93194a73ee79205","IPY_MODEL_9d696b095c544f81a655a799a9bc8c38"],"layout":"IPY_MODEL_e04e72e73a3e4d25bfc9468143befa5d"}},"6e24c15ccc7940b2b2d7a2c80fc0eeb1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e67c096fa7a84621abc18b46f09b8776","placeholder":"​","style":"IPY_MODEL_e8f278acc015410dad174563c740d356","value":"special_tokens_map.json: 100%"}},"d328f8581a624833b93194a73ee79205":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5dfcb101f32546f08d48132859af92bd","max":244,"min":0,"orientation":"horizontal","style":"IPY_MODEL_835b5bacb55b4d8d89202aab65d934cf","value":244}},"9d696b095c544f81a655a799a9bc8c38":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6657113651af4ea48c988f1d5eb9dde3","placeholder":"​","style":"IPY_MODEL_1f2266532edf40febbf320db1c7822b8","value":" 244/244 [00:00&lt;00:00, 7.63kB/s]"}},"e04e72e73a3e4d25bfc9468143befa5d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e67c096fa7a84621abc18b46f09b8776":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8f278acc015410dad174563c740d356":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5dfcb101f32546f08d48132859af92bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"835b5bacb55b4d8d89202aab65d934cf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6657113651af4ea48c988f1d5eb9dde3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f2266532edf40febbf320db1c7822b8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i19wLMG7de9s","executionInfo":{"status":"ok","timestamp":1731907507144,"user_tz":-540,"elapsed":16307,"user":{"displayName":"이서영","userId":"14365237330441168940"}},"outputId":"6571ff33-232c-4229-c986-5405a7d6b6ca"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# 필요한 라이브러리 import\n","import pandas as pd\n","import torch\n","from transformers import BertTokenizer, BertModel\n","from sklearn.metrics.pairwise import cosine_similarity"],"metadata":{"id":"VL9dJz-1vCoR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터 불러오기\n","file_path = '/content/drive/MyDrive/result_meal.csv'  # Colab에 파일을 업로드하고 이 경로를 확인하세요\n","df = pd.read_csv(file_path)\n","\n","# 'text_review' 컬럼이 있는지 확인\n","if 'text_review' not in df.columns:\n","    print(\"Error: 'text_review' 컬럼이 데이터프레임에 없습니다.\")\n","else:\n","    # 사용자 텍스트 입력 받기\n","    user_input = input(\"비교할 텍스트를 입력하세요: \")\n","\n","    # KOBERT 모델과 토크나이저 로드\n","    tokenizer = BertTokenizer.from_pretrained('skt/kobert-base-v1')\n","    model = BertModel.from_pretrained('skt/kobert-base-v1')\n","\n","    # 텍스트를 임베딩하는 함수 정의\n","    def embed_text(text):\n","        tokens = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n","        with torch.no_grad():\n","            output = model(**tokens)\n","        # 일반적으로 BERT의 마지막 레이어의 CLS 토큰 벡터를 임베딩으로 사용\n","        embedding = output.last_hidden_state[:, 0, :].numpy()\n","        return embedding\n","\n","    # 입력 텍스트와 데이터프레임의 리뷰 텍스트들을 임베딩\n","    user_embedding = embed_text(user_input)\n","\n","    reviews_embedding = [embed_text(review) for review in df['text_review'].tolist()]\n","\n","    # 코사인 유사도 계산\n","    similarity_scores = [cosine_similarity(user_embedding, review_emb)[0][0] for review_emb in reviews_embedding]\n","\n","    # 유사도 점수를 데이터프레임에 추가\n","    df['similarity_score'] = similarity_scores\n","\n","    # 유사도 점수가 높은 순으로 정렬\n","    df_sorted = df.sort_values(by='similarity_score', ascending=False)\n","\n","    # 상위 5개 결과 출력\n","    print(\"입력 텍스트와 유사한 리뷰 상위 4개:\")\n","    print(df_sorted[['store_name', 'text_review', 'similarity_score']].head(4))\n","\n","    # 정렬된 결과를 CSV로 저장 (원하는 경우)\n","    df_sorted.to_csv('/content/sorted_similarity_scores.csv', index=False)\n","    print(\"유사도 점수가 포함된 결과가 sorted_similarity_scores.csv 파일로 저장되었습니다.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":614,"referenced_widgets":["32c51f3aa521454dbfbf0730dbad251a","a9683202c9df46c09792e94a932aaf54","9f302c2608604de2978b88961a9b0d9d","e49f0eeb0b274a6cb19cad2ac910b28c","ee2aa3442d53498f99ed6ee218621040","5d2dcd8a40ed479296d4a207ac5a10d0","9722e952a4cc49deb1cfe46ad0f73000","306180ab90c24cb08a07d0fade54ea8c","131f5d857e254e8fbaf667b1e9ad75de","83c29790a92b4df7a149cc6c363cc02c","8d5de93162054eafa5ff484707e6d627","564ae84188c24b66bd86427b1281eaa2","6e24c15ccc7940b2b2d7a2c80fc0eeb1","d328f8581a624833b93194a73ee79205","9d696b095c544f81a655a799a9bc8c38","e04e72e73a3e4d25bfc9468143befa5d","e67c096fa7a84621abc18b46f09b8776","e8f278acc015410dad174563c740d356","5dfcb101f32546f08d48132859af92bd","835b5bacb55b4d8d89202aab65d934cf","6657113651af4ea48c988f1d5eb9dde3","1f2266532edf40febbf320db1c7822b8"]},"id":"tBspJlJTuYro","executionInfo":{"status":"error","timestamp":1731907724122,"user_tz":-540,"elapsed":60244,"user":{"displayName":"이서영","userId":"14365237330441168940"}},"outputId":"5a855e99-0a64-482f-d978-b217ff8796f9"},"execution_count":3,"outputs":[{"name":"stdout","output_type":"stream","text":["비교할 텍스트를 입력하세요: 조용하고 아늑한 분위기\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/432 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32c51f3aa521454dbfbf0730dbad251a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/244 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"564ae84188c24b66bd86427b1281eaa2"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n","The class this function is called from is 'BertTokenizer'.\n"]},{"output_type":"error","ename":"TypeError","evalue":"stat: path should be string, bytes, os.PathLike or integer, not NoneType","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-d45114499c97>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# KOBERT 모델과 토크나이저 로드\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skt/kobert-base-v1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skt/kobert-base-v1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2211\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"loading file {file_path} from cache at {resolved_vocab_files[file_id]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2213\u001b[0;31m         return cls._from_pretrained(\n\u001b[0m\u001b[1;32m   2214\u001b[0m             \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2215\u001b[0m             \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2445\u001b[0m         \u001b[0;31m# Instantiate the tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2446\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2447\u001b[0;31m             \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2448\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mimport_protobuf_decode_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2449\u001b[0m             logger.info(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_file, do_lower_case, do_basic_tokenize, never_split, unk_token, sep_token, pad_token, cls_token, mask_token, tokenize_chinese_chars, strip_accents, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     ):\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m             raise ValueError(\n\u001b[1;32m    116\u001b[0m                 \u001b[0;34mf\"Can't find a vocabulary file at path '{vocab_file}'. To load the vocabulary from a Google pretrained\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/genericpath.py\u001b[0m in \u001b[0;36misfile\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;34m\"\"\"Test whether a path is a regular file\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: stat: path should be string, bytes, os.PathLike or integer, not NoneType"]}]},{"cell_type":"code","source":["\n","# 필요한 라이브러리 import\n","import pandas as pd\n","from sentence_transformers import SentenceTransformer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","# 데이터 불러오기\n","file_path = '/content/drive/MyDrive/result_meal.csv'  # Colab에 파일을 업로드하고 이 경로를 확인하세요\n","df = pd.read_csv(file_path)"],"metadata":{"id":"tHR0s37ob5uk","executionInfo":{"status":"ok","timestamp":1731908141253,"user_tz":-540,"elapsed":322,"user":{"displayName":"이서영","userId":"14365237330441168940"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["import re\n","\n","# 이모티콘과 특수 문자를 제거하는 함수\n","def clean_text(text):\n","    # 입력값이 NaN이면 그대로 반환\n","    if pd.isna(text):\n","        return text\n","\n","    # 유니코드 이모티콘 제거\n","    text = re.sub(r'[\\U00010000-\\U0010ffff]', '', text)\n","    # 특수 문자와 공백을 제거하고 문장 부호를 최소화\n","    text = re.sub(r'[^\\w\\s]', ' ', text)\n","    # 연속된 공백을 하나의 공백으로 압축\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","    return text\n","\n","# 텍스트 정제\n","df['cleaned_text'] = df['text_review'].apply(clean_text)"],"metadata":{"id":"qnrcM1Mew4nu","executionInfo":{"status":"ok","timestamp":1731908477567,"user_tz":-540,"elapsed":334,"user":{"displayName":"이서영","userId":"14365237330441168940"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# 'text_review' 컬럼이 있는지 확인\n","if 'text_review' not in df.columns:\n","    print(\"Error: 'text_review' 컬럼이 데이터프레임에 없습니다.\")\n","else:\n","    # NaN 값을 제거하거나 빈 문자열로 대체\n","    df['cleaned_text'] = df['cleaned_text'].fillna('')\n","\n","    # 사용자 텍스트 입력 받기\n","    user_input = input(\"비교할 텍스트를 입력하세요: \")\n","\n","    # 모델 로드 (SBERT 사용)\n","    model = SentenceTransformer('all-MiniLM-L6-v2')\n","\n","    # 입력 텍스트와 데이터프레임의 리뷰 텍스트들을 임베딩\n","    user_embedding = model.encode([user_input])\n","    reviews_embedding = model.encode(df['cleaned_text'].tolist())\n","\n","    # 코사인 유사도 계산\n","    similarity_scores = cosine_similarity(user_embedding, reviews_embedding)[0]\n","\n","    # 유사도 점수를 데이터프레임에 추가\n","    df['similarity_score'] = similarity_scores\n","\n","    # 유사도 점수가 높은 순으로 정렬\n","    df_sorted = df.sort_values(by='similarity_score', ascending=False)\n","\n","    # 상위 5개 결과 출력\n","    print(\"입력 텍스트와 유사한 리뷰 상위 5개:\")\n","    print(df_sorted[['store_name', 'text_review', 'similarity_score']].head())\n","\n","    # 정렬된 결과를 CSV로 저장 (원하는 경우)\n","    df_sorted.to_csv('/content/sorted_similarity_scores.csv', index=False)\n","    print(\"유사도 점수가 포함된 결과가 sorted_similarity_scores.csv 파일로 저장되었습니다.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eRCJhqDnwuo1","executionInfo":{"status":"ok","timestamp":1731908592462,"user_tz":-540,"elapsed":106110,"user":{"displayName":"이서영","userId":"14365237330441168940"}},"outputId":"48b3159b-c405-442b-a65e-7ca6ba98d9da"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["비교할 텍스트를 입력하세요: 해장하기 좋은 국물\n","입력 텍스트와 유사한 리뷰 상위 5개:\n","     store_name                        text_review  similarity_score\n","2      육쌈냉면 숙대점                   냉면이랑 고기 둘다 맛있어요!          0.837308\n","587   타코비 숙명여대점        사장님 너무 친절하시고 타코야끼 너무 맛있어요:)          0.836435\n","1038   남영동양문 본점  음식 다 맛있고 친절하시고 좋아요! 곧 또 방문 예정입니다🤞          0.833320\n","1328       카페시바            항상 궁금했는데, 너무너무 맛있었어요 ☺️          0.828620\n","539   홍곱창 숙명여대점                    소금구이 막창 너무 맛있어요          0.826579\n","유사도 점수가 포함된 결과가 sorted_similarity_scores.csv 파일로 저장되었습니다.\n"]}]},{"cell_type":"code","source":["    # 유사도 점수가 높은 순으로 정렬\n","    df_sorted = df.sort_values(by='similarity_score', ascending=False)\n","\n","    # store_name별로 유사도 점수 상위 5개의 평균을 계산\n","    top_5_avg_similarity = (\n","        df_sorted.groupby('store_name')\n","        .apply(lambda x: x.nlargest(5, 'similarity_score')['similarity_score'].mean())\n","        .reset_index(name='top_5_avg_similarity')\n","    )\n","\n","    # 평균 유사도 점수가 가장 높은 5개의 store_name 출력\n","    top_5_avg_similarity_sorted = top_5_avg_similarity.sort_values(by='top_5_avg_similarity', ascending=False).head(5)\n","\n","    print(\"유사도 점수 상위 5개의 평균이 가장 높은 상위 5개의 가게:\")\n","    print(top_5_avg_similarity_sorted)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wcOWMpPBz3u2","executionInfo":{"status":"ok","timestamp":1731908834801,"user_tz":-540,"elapsed":363,"user":{"displayName":"이서영","userId":"14365237330441168940"}},"outputId":"e11d6bd5-ea0c-48fc-b26e-72b3b7d5e82f"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["유사도 점수 상위 5개의 평균이 가장 높은 상위 5개의 가게:\n","     store_name  top_5_avg_similarity\n","35   빨봉분식 숙명여대점              0.790326\n","48    아리랑노점 숙대점              0.789264\n","26  미스터카츠 숙대입구점              0.779017\n","66    타코비 숙명여대점              0.778816\n","38         살팀보카              0.777568\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-13-42b4cb43be4b>:7: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  .apply(lambda x: x.nlargest(5, 'similarity_score')['similarity_score'].mean())\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZMG723L6bh3G","executionInfo":{"status":"ok","timestamp":1731333273225,"user_tz":-540,"elapsed":93312,"user":{"displayName":"이서영","userId":"14365237330441168940"}},"outputId":"5860ecbf-304b-43dd-9fd7-386dad9febde"},"outputs":[{"name":"stdout","output_type":"stream","text":["비교할 텍스트를 입력하세요: 공부하기 좋아요\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["입력 텍스트와 유사한 리뷰 상위 5개:\n","      store_name            combined_text  similarity_score\n","22   벤티프레소 숙명여대점                 가성비 최고예요          0.884818\n","243    미드스트오브플로우    독특한 느낌조용한 분위기2층은 사용불가          0.848002\n","35   벤티프레소 숙명여대점            맛있어요 친구랑 자주가용          0.822986\n","381         을의커피               카공하기 좋아요!!          0.817015\n","118       카페 퓨엔테   디저트도 맛있고 좋아여자주와서 놀고갑니당          0.788587\n","유사도 점수가 포함된 결과가 sorted_similarity_scores.csv 파일로 저장되었습니다.\n"]}],"source":["# 필요한 라이브러리 import\n","import pandas as pd\n","from sentence_transformers import SentenceTransformer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","# 'text_review' 컬럼이 있는지 확인\n","if 'text_review' not in df.columns:\n","    print(\"Error: 'text_review' 컬럼이 데이터프레임에 없습니다.\")\n","else:\n","    # 사용자 텍스트 입력 받기\n","    user_input = input(\"비교할 텍스트를 입력하세요: \")\n","\n","    # 키워드와 리뷰를 하나의 텍스트로 결합\n","    def combine_text(row):\n","        keywords = ' '.join([str(row[col]) for col in ['visit_keyword_1', 'visit_keyword_2', 'visit_keyword_3', 'visit_keyword_4',\n","                                                       'review_keyword_1', 'review_keyword_2', 'review_keyword_3', 'review_keyword_4', 'review_keyword_5'] if col in row])\n","        return f\"{keywords} {row['text_review']}\"\n","\n","    # 새로운 컬럼으로 키워드와 리뷰 결합 텍스트 생성\n","    df['combined_text'] = df.apply(combine_text, axis=1)\n","\n","    # 모델 로드 (SBERT 사용)\n","    model = SentenceTransformer('all-MiniLM-L6-v2')\n","\n","    # 입력 텍스트와 데이터프레임의 결합 텍스트들을 임베딩\n","    user_embedding = model.encode([user_input])\n","    combined_embeddings = model.encode(df['combined_text'].tolist())\n","\n","    # 코사인 유사도 계산\n","    similarity_scores = cosine_similarity(user_embedding, combined_embeddings)[0]\n","\n","    # 유사도 점수를 데이터프레임에 추가\n","    df['similarity_score'] = similarity_scores\n","\n","    # 유사도 점수가 높은 순으로 정렬\n","    df_sorted = df.sort_values(by='similarity_score', ascending=False)\n","\n","    # 상위 5개 결과 출력\n","    print(\"입력 텍스트와 유사한 리뷰 상위 5개:\")\n","    print(df_sorted[['store_name', 'combined_text', 'similarity_score']].head())\n","\n","    # 정렬된 결과를 CSV로 저장 (원하는 경우)\n","    df_sorted.to_csv('/content/sorted_similarity_scores.csv', index=False)\n","    print(\"유사도 점수가 포함된 결과가 sorted_similarity_scores.csv 파일로 저장되었습니다.\")\n"]}]}